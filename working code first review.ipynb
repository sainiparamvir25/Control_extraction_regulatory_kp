{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract end page until which TOC is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber\n",
    "from typing import Optional\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "def find_table_of_contents_end_page(pdf_path: str, chat_model: AzureChatOpenAI, batch_size: int = 10, max_pages: int = 50) -> int:\n",
    "    \"\"\"\n",
    "    Detects the last page number where the Table of Contents (TOC) appears in a PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        chat_model (AzureChatOpenAI): LangChain AzureChatOpenAI instance.\n",
    "        batch_size (int): Number of pages to process in each LLM call.\n",
    "        max_pages (int): Maximum number of pages to scan (default 50).\n",
    "        \n",
    "    Returns:\n",
    "        int: Last page number where the Table of Contents is found.\n",
    "    \"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total_pages = len(pdf.pages)\n",
    "        pages_to_check = min(total_pages, max_pages)\n",
    "\n",
    "        for start in range(1, pages_to_check + 1, batch_size):\n",
    "            end = min(start + batch_size - 1, pages_to_check)\n",
    "            batch_pages = []\n",
    "            for i in range(start - 1, end):\n",
    "                text = pdf.pages[i].extract_text() or \"\"\n",
    "                batch_pages.append(f\"\\n--- PAGE {i + 1} ---\\n{text}\")\n",
    "\n",
    "            combined_text = \"\\n\".join(batch_pages)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "SYSTEM:\n",
    "You are an expert in document structure analysis.\n",
    "\n",
    "INSTRUCTION:\n",
    "You are given a batch of pages from the start of a PDF document. Analyze and determine up to which page the Table of Contents (TOC) continues.\n",
    "\n",
    "RULES:\n",
    "- Identify if any of the pages contain Table of Contents using clues like: \n",
    "  * headings such as \"Contents\", \"Table of Contents\"\n",
    "  * dot leaders (e.g., \"1.1 Overview .......... 5\")\n",
    "  * chapter or section listings with page numbers\n",
    "\n",
    "RETURN FORMAT:\n",
    "- Return only the page number **after which** the Table of Contents ends.\n",
    "- If TOC is not present in this batch, return \"NONE\".\n",
    "- If TOC continues beyond this batch, return \"CONTINUES\".\n",
    "\n",
    "PDF PAGES CONTENT:\n",
    "{combined_text}\n",
    "\"\"\"\n",
    "\n",
    "            result = chat_model.invoke(prompt).content.strip()\n",
    "\n",
    "            if result.isdigit():\n",
    "                return int(result)\n",
    "            elif result.upper() == \"NONE\":\n",
    "                return max(1, start - 1)\n",
    "            elif result.upper() == \"CONTINUES\":\n",
    "                continue\n",
    "\n",
    "        # Fallback if TOC end isn't confidently found\n",
    "        return 3\n",
    "\n",
    "\n",
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_key=\"13p7qJwQxxNSbetSXlCoBpNQoJBIIMY35fUIQrdZ7ji7weqpM6K2JQQJ99BFACHYHv6XJ3w3AAAAACOGQaS7\", \n",
    "    openai_api_base=\"https://param-mc26b6rc-eastus2.cognitiveservices.azure.com/\",\n",
    "    openai_api_version=\"2025-01-01-preview\",\n",
    "    deployment_name=\"o4-mini\",\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "pdf_path = \"ia.pdf\"\n",
    "toc_end_page = find_table_of_contents_end_page(pdf_path, chat_model)\n",
    "print(f\"üìò Table of Contents ends at page: {toc_end_page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sections from TOC which contains controles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pdfplumber\n",
    "from typing import List, Dict\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "def extract_control_sections_from_toc(\n",
    "    pdf_path: str,\n",
    "    toc_end_page: int,\n",
    "    chat_model: AzureChatOpenAI\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts sections from the Table of Contents that contain actual controls,\n",
    "    with section name, start page, and end page.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the regulatory PDF file.\n",
    "        toc_end_page (int): Page number where the Table of Contents ends.\n",
    "        chat_model (AzureChatOpenAI): Azure OpenAI LLM instance.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Each dictionary has 'section_name', 'start_page', 'end_page'.\n",
    "    \"\"\"\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        toc_text = []\n",
    "        for i in range(toc_end_page):\n",
    "            text = pdf.pages[i].extract_text() or \"\"\n",
    "            toc_text.append(f\"\\n--- PAGE {i + 1} ---\\n{text}\")\n",
    "\n",
    "        combined_toc = \"\\n\".join(toc_text)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "SYSTEM:\n",
    "You are a cybersecurity compliance analyst and expert in regulatory frameworks like ISO 27001, NIST 800-53, and UAE IA Regulation.\n",
    "You are analyzing the Table of Contents (TOC) from a regulatory document to identify which sections are likely to contain security or compliance **controls**.\n",
    "\n",
    "INSTRUCTION:\n",
    "Based on the TOC content provided, extract the list of sections or subsections that are likely to contain actual regulatory controls.\n",
    "\n",
    "RULES:\n",
    "1. Look for keywords like \"Controls\", \"Security Requirements\", \"Information Assurance\", \"Risk Treatment\", \n",
    "\"Security Measures\", \"Technical Controls\", \"Management Controls\".\n",
    "2. A section may contain controls directly or indirectly via child sections ‚Äî include both if applicable.\n",
    "3. For each identified section, extract:\n",
    "   - `section_name`: Full name of the section or sub-section\n",
    "   - `start_page`: The page number the section starts from (as per TOC)\n",
    "   - `end_page`: The page number it ends on (i.e., just one page before the very next listed section).\n",
    "4. end_page of all the section_names is always the start_page of very next section_name. Do never deviate from this.\n",
    "5. If multiple subsections contain controls, include each as a separate entry.\n",
    "6. If a single large section contains all controls, return just one dictionary entry. \n",
    "7. If no sections contain controls, return an empty list.\n",
    "8. Keep end page od any sections as start page of the very next sections or subsection.\n",
    "9. Always give section and subsections in the output. \n",
    "10. Ensure that the output dictionary includes both sections and all of their corresponding subsections. No control-related section or subsection should be omitted.\n",
    "11. Between the identified start and end pages, do not skip or exclude any sections or subsections. All intermediate sections and subsections must be included in the output.\n",
    "12. Only include sections or subsections that actually define controls. Exclude any content that is only introductory, explanatory, summarizing, or meta in nature ‚Äî such as summaries, mappings, methodologies, or overviews. \n",
    "13. Do not include sections or subsections which will not have actual controls like structure or how controles are made.\n",
    "\n",
    "FORMAT:\n",
    "Return only a **JSON list** of dictionaries with this structure:\n",
    "[\n",
    "  {{\n",
    "    \"section_name\": \"Security Controls\",\n",
    "    \"start_page\": 33,\n",
    "    \"end_page\": 89\n",
    "  }},\n",
    "  {{\n",
    "    \"section_name\": \"Annex A: ISO 27001 Controls\",\n",
    "    \"start_page\": 90,\n",
    "    \"end_page\": 112\n",
    "  }}\n",
    "]\n",
    "\n",
    "EXAMPLES:\n",
    "If the TOC says:\n",
    "- \"Chapter 4: Information Security Controls ............25\"\n",
    "- \"Chapter 5: Physical Security Controls .............. 40\"\n",
    "- \"Annex A: Control Catalogue ......................... 60\"\n",
    "You might output:\n",
    "[\n",
    "  {{\n",
    "    \"section_name\": \"Chapter 4: Information Security Controls\",\n",
    "    \"start_page\": 25,\n",
    "    \"end_page\": 39\n",
    "  }},\n",
    "  {{\n",
    "    \"section_name\": \"Chapter 5: Physical Security Controls\",\n",
    "    \"start_page\": 40,\n",
    "    \"end_page\": 59\n",
    "  }},\n",
    "  {{\n",
    "    \"section_name\": \"Annex A: Control Catalogue\",\n",
    "    \"start_page\": 60,\n",
    "    \"end_page\": 80\n",
    "  }}\n",
    "]\n",
    "\n",
    "Most important rule: \n",
    "1. end_page of all the section_names is always the start_page of very next section_name. Do never deviate from this.\n",
    "2. Do not include sections or subsections which will not have actual controls like structure or how controles are made.\n",
    "TOC CONTENT:\n",
    "{combined_toc}\n",
    "\"\"\"\n",
    "\n",
    "    result = chat_model.invoke(prompt).content.strip()\n",
    "\n",
    "    # Try parsing it as list of dictionaries\n",
    "    import json\n",
    "    try:\n",
    "        control_sections = json.loads(result)\n",
    "        if isinstance(control_sections, list):\n",
    "            return control_sections\n",
    "        else:\n",
    "            raise ValueError(\"Expected list output from LLM\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ö†Ô∏è Failed to parse LLM output as JSON.\")\n",
    "        print(\"Raw Output:\", result)\n",
    "        return []\n",
    "\n",
    "\n",
    "pdf_path = \"ia.pdf\"\n",
    "toc_end_page = 3  # Replace with actual detected value\n",
    "\n",
    "sections = extract_control_sections_from_toc(pdf_path, toc_end_page, chat_model)\n",
    "\n",
    "for section in sections:\n",
    "    print(f\"üìò {section['section_name']} | Pages {section['start_page']} to {section['end_page']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the pages are less then directly extract the controles csv (For <50 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Extract Text from PDF in Plain Format\n",
    "# -----------------------------\n",
    "def extract_pdf_content_plaintext(pdf_path: str, start_page: int, end_page: int) -> str:\n",
    "    pages = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        total = len(pdf.pages)\n",
    "        if start_page < 1 or end_page > total or start_page > end_page:\n",
    "            raise ValueError(f\"Invalid page range: 1 <= start <= end <= {total}\")\n",
    "\n",
    "        for idx in range(start_page - 1, end_page):\n",
    "            page = pdf.pages[idx]\n",
    "            text = page.extract_text() or \"\"\n",
    "\n",
    "            # Format plain text content\n",
    "            page_text = f\"Page: {idx + 1}\\nContent:\\n{text.strip()}\"\n",
    "\n",
    "            # Add tables\n",
    "            tables_text = \"\"\n",
    "            for table in page.extract_tables():\n",
    "                if table:\n",
    "                    tables_text += \"\\nTable:\\n\"\n",
    "                    for row in table:\n",
    "                        tables_text += \" | \".join(cell.strip() if cell else \"\" for cell in row) + \"\\n\"\n",
    "            page_text += f\"\\n{tables_text.strip()}\"\n",
    "\n",
    "            pages.append(page_text.strip())\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(pages)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Build PromptTemplate\n",
    "# -----------------------------\n",
    "PROMPT_TEMPLATE = \"\"\"SYSTEM:\n",
    "You are a PDF parsing and compliance‚Äëextraction assistant specialized in cybersecurity standards. \n",
    "Your task is to read the full text of one or more PDF standards documents (e.g. UAE IA Regulation, ISO¬†27001, \n",
    "NIST CSF, NISA Guidelines) and output a structured list of all controls, preserving their hierarchical numbering \n",
    "and labels.\n",
    "\n",
    "RULES:\n",
    "\n",
    "1. Identify each **Domain** (sometimes labeled ‚ÄúSection‚Äù, ‚ÄúClause‚Äù, or ‚ÄúCategory‚Äù) by its numbering and \n",
    "title (e.g. ‚Äú1.0 Information Security Management‚Äù or ‚ÄúAnnex A ‚Äì Security Controls‚Äù).\n",
    "2. Within each Domain, identify each **Sub‚ÄëDomain** (sometimes called ‚ÄúSub‚ÄëClause‚Äù, ‚ÄúControl Family‚Äù, or ‚ÄúArea‚Äù) \n",
    "by its numbering and title (e.g. ‚Äú1.1 Risk Assessment‚Äù or ‚ÄúA.5 Access Control‚Äù).\n",
    "3. For every control under a Sub‚ÄëDomain, extract:\n",
    "\n",
    "   * **control\\_number**: the exact hierarchical number (e.g. ‚Äú1.1.2‚Äù, ‚ÄúA.5.1‚Äù).\n",
    "   * **control\\_title**: the short control name or label.\n",
    "   * **description**: the full descriptive text of the control.\n",
    "4. Domains and Sub‚ÄëDomains may span pages. Continue assigning controls to the most recent Domain/Sub‚ÄëDomain until a \n",
    "new one appears.\n",
    "5. Different documents may use different labels (e.g. ‚ÄúDomain‚Äù vs. ‚ÄúSection‚Äù, ‚ÄúSub‚ÄëDomain‚Äù vs. ‚ÄúClause‚Äù), but **the \n",
    "numbering hierarchy is the single source of truth.** Always use the numeric/order prefixes to determine hierarchy.\n",
    "6. Output a **single JSON array** of objects, each with these keys:\n",
    "   ‚Ä¢ domain\n",
    "   ‚Ä¢ sub\\_domain\n",
    "   ‚Ä¢ control\\_number\n",
    "   ‚Ä¢ control\\_title\n",
    "   ‚Ä¢ description\n",
    "7. Do not output any additional text‚Äîonly the JSON array. If a Domain or Sub‚ÄëDomain has no explicit title but only \n",
    "a number, use the number as the name (e.g. `\"sub_domain\": \"A.6\"`).\n",
    "8. Remember you need to consider entire use content and create the json array by considering the entire document.\n",
    "USER CONTENT:\n",
    "{content}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Set Up Azure OpenAI Chat Model\n",
    "# -----------------------------\n",
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_key=\"13p7qJwQxxNSbetSXlCoBpNQoJBIIMY35fUIQrdZ7ji7weqpM6K2JQQJ99BFACHYHv6XJ3w3AAAAACOGQaS7\", \n",
    "    openai_api_base=\"https://param-mc26b6rc-eastus2.cognitiveservices.azure.com/\",\n",
    "    openai_api_version=\"2025-01-01-preview\",\n",
    "    deployment_name=\"o4-mini\",\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Run the Extraction\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"iso.pdf\"\n",
    "    start_page = 1\n",
    "    end_page = 26 # Adjust as needed\n",
    "\n",
    "    content = extract_pdf_content_plaintext(pdf_path, start_page, end_page)\n",
    "    formatted_prompt = prompt.format(content=content)\n",
    "    response = chat_model.invoke(formatted_prompt)\n",
    "\n",
    "    print(response.content)\n",
    "\n",
    "    import json\n",
    "    import csv\n",
    "\n",
    "    # Step 1: Extract JSON string from AIMessage\n",
    "    json_str = response.content\n",
    "\n",
    "    # Step 2: Convert string to list of dictionaries\n",
    "    try:\n",
    "        control_data = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(\"The model response is not valid JSON. Check formatting.\") from e\n",
    "\n",
    "    # Step 3: Write to CSV\n",
    "    output_file = \"ec.csv\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"domain\", \"sub_domain\", \"control_number\", \"control_title\", \"description\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(control_data)\n",
    "\n",
    "    print(f\"‚úÖ CSV saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If pages are more than 50 pages then we use same prompt but we pass chunks from the complete content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ----------------------------- CONFIG\n",
    "OUTPUT_FILE = \"extracted_controls_v5.csv\"\n",
    "\n",
    "\n",
    "# ----------------------------- CONTROL EXTRACTION PROMPT\n",
    "PROMPT_TEMPLATE = \"\"\"SYSTEM:\n",
    "You are a cybersecurity compliance parser extracting controls from regulatory documents (like UAE IA, ISO 27001, etc).\n",
    "\n",
    "OBJECTIVE:\n",
    "Parse the provided document chunk and extract any **security controls** with their full hierarchy.\n",
    "\n",
    "RULES:\n",
    "1. Identify each Domain and Sub‚ÄëDomain using numbering (e.g. ‚Äú1.0‚Äù, ‚ÄúA.5‚Äù).\n",
    "2. Under each Sub‚ÄëDomain, extract:\n",
    "   - control_number: e.g. ‚ÄúA.5.1.2‚Äù\n",
    "   - control_title: short heading\n",
    "   - description: full text\n",
    "\n",
    "3. If no controls are present in this content, return an **empty JSON array**: []\n",
    "\n",
    "4. Output format (strict):\n",
    "[\n",
    "  {{\n",
    "    \"domain\": \"A.5: Planning & Testing\",\n",
    "    \"sub_domain\": \"A.5.1: Context and the leadership\",\n",
    "    \"control_number\": \"A.5.1.1\",\n",
    "    \"control_title\": \"Access Control Policy\",\n",
    "    \"description\": \"Establish and review access control policies. this should contain controles and sub controles clubbed together\"\n",
    "  }}\n",
    "  ...\n",
    "]\n",
    "\n",
    "REMEMBER:\n",
    "- Use section numbering hierarchy to infer structure.\n",
    "- Do not include any explanation, headers, or notes‚Äîreturn JSON array only.\n",
    "- If you dont find domain, subdomain, controles number, control title and description then do not create the output for that content.\n",
    "\n",
    "USER CONTENT:\n",
    "{content}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# ----------------------------- EXTRACT TEXT + TABLES\n",
    "def extract_pdf_chunk_content(pdf_path: str, start_page: int, end_page: int) -> str:\n",
    "    pages = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for idx in range(start_page - 2, end_page):\n",
    "            page = pdf.pages[idx]\n",
    "            text = page.extract_text() or \"\"\n",
    "            page_text = f\"Page {idx + 1}:\\n{text.strip()}\"\n",
    "\n",
    "            tables_text = \"\"\n",
    "            for table in page.extract_tables():\n",
    "                if table:\n",
    "                    tables_text += \"\\nTable:\\n\"\n",
    "                    for row in table:\n",
    "                        tables_text += \" | \".join(cell.strip() if cell else \"\" for cell in row) + \"\\n\"\n",
    "\n",
    "            pages.append(page_text + \"\\n\" + tables_text.strip())\n",
    "    return \"\\n\\n---\\n\\n\".join(pages)\n",
    "\n",
    "# ----------------------------- PROCESS EACH SECTION\n",
    "def process_and_append_section(\n",
    "    pdf_path: str,\n",
    "    section: Dict[str, int],\n",
    "    chat_model: AzureChatOpenAI,\n",
    "    output_csv_path: str\n",
    "):\n",
    "    section_text = extract_pdf_chunk_content(pdf_path, section['start_page'], section['end_page'])\n",
    "    formatted_prompt = prompt.format(content=section_text)\n",
    "    response = chat_model.invoke(formatted_prompt).content.strip()\n",
    "    time.sleep(100)\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(response)\n",
    "        if not parsed_json:\n",
    "            print(f\"‚õî No controls found in section: {section['section_name']}\")\n",
    "            return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ö†Ô∏è JSON parsing failed for section: {section['section_name']}\")\n",
    "        print(response)\n",
    "        return\n",
    "\n",
    "    print(f\"‚úÖ Found {len(parsed_json)} controls in section: {section['section_name']}\")\n",
    "\n",
    "    # Append to CSV\n",
    "    with open(output_csv_path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"domain\", \"sub_domain\", \"control_number\", \"control_title\", \"description\"])\n",
    "        if f.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(parsed_json)\n",
    "\n",
    "# ----------------------------- MAIN DRIVER\n",
    "def extract_controls_from_sections(pdf_path: str, sections: List[Dict], chat_model: AzureChatOpenAI):\n",
    "    for section in sections:\n",
    "        process_and_append_section(pdf_path, section, chat_model, OUTPUT_FILE)\n",
    "\n",
    "# ----------------------------- USAGE EXAMPLE\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"ia.pdf\"\n",
    "\n",
    "    # Example TOC output from previous step\n",
    "    control_sections = ps_sections\n",
    "\n",
    "    extract_controls_from_sections(pdf_path, control_sections, chat_model)\n",
    "    print(f\"\\nüìÑ Final output written to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final code for controles similarity using vector db since controles are more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "PRIMARY_CSV = \"extracted_controls_v2.csv\"\n",
    "SECONDARY_CSV = \"extracted_controls.csv\"\n",
    "OUTPUT_CSV = \"uae_vs_iso_mapped_rag.csv\"\n",
    "\n",
    "PRIMARY_PREFIX = \"UAE_IA\"\n",
    "SECONDARY_PREFIX = \"ISO\"\n",
    "TOP_K = 5\n",
    "\n",
    "# -----------------------------\n",
    "# LangChain Chat Model (Azure)\n",
    "# -----------------------------\n",
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_key=\"13p7qJwQxxNSbetSXlCoBpNQoJBIIMY35fUIQrdZ7ji7weqpM6K2JQQJ99BFACHYHv6XJ3w3AAAAACOGQaS7\", \n",
    "    openai_api_base=\"https://param-mc26b6rc-eastus2.cognitiveservices.azure.com/\",\n",
    "    openai_api_version=\"2025-01-01-preview\",\n",
    "    deployment_name=\"o4-mini\",\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Embedding Setup\n",
    "# -----------------------------\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_key=\"13p7qJwQxxNSbetSXlCoBpNQoJBIIMY35fUIQrdZ7ji7weqpM6K2JQQJ99BFACHYHv6XJ3w3AAAAACOGQaS7\",\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=\"https://param-mc26b6rc-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15\"\n",
    ")\n",
    "\n",
    "EMBEDDING_DEPLOYMENT = \"text-embedding-ada-002\"\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    response = embedding_client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=EMBEDDING_DEPLOYMENT\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt Template\n",
    "# -----------------------------\n",
    "MATCHING_PROMPT_TEMPLATE = \"\"\"You are an expert in cybersecurity compliance mappings.\n",
    "\n",
    "Your task is to determine whether the following two regulatory controls are semantically aligned ‚Äî meaning they have the same intent, coverage, or enforcement scope.\n",
    "\n",
    "Please read both controls carefully and answer only with:\n",
    "Yes ‚Äî if they are aligned\n",
    "No ‚Äî if they are not\n",
    "\n",
    "Primary Control:\n",
    "Domain: {primary_domain}\n",
    "Sub-domain: {primary_sub_domain}\n",
    "Number: {primary_control_number}\n",
    "Title: {primary_control_title}\n",
    "Description: {primary_description}\n",
    "\n",
    "Secondary Control:\n",
    "Domain: {secondary_domain}\n",
    "Sub-domain: {secondary_sub_domain}\n",
    "Number: {secondary_control_number}\n",
    "Title: {secondary_control_title}\n",
    "Description: {secondary_description}\n",
    "\n",
    "Are these controls aligned? Respond with Yes or No. Do not give any explaination or any placeholders.\n",
    "\n",
    "\n",
    "PRIMARY CONTROL:\n",
    "Domain: {primary_domain}\n",
    "Sub-domain: {primary_sub_domain}\n",
    "Control Number: {primary_control_number}\n",
    "Title: {primary_control_title}\n",
    "Description: {primary_description}\n",
    "\n",
    "SECONDARY CONTROL:\n",
    "Domain: {secondary_domain}\n",
    "Sub-domain: {secondary_sub_domain}\n",
    "Control Number: {secondary_control_number}\n",
    "Title: {secondary_control_title}\n",
    "Description: {secondary_description}\n",
    "\"\"\"\n",
    "\n",
    "match_prompt = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"primary_domain\", \"primary_sub_domain\", \"primary_control_number\",\n",
    "        \"primary_control_title\", \"primary_description\",\n",
    "        \"secondary_domain\", \"secondary_sub_domain\", \"secondary_control_number\",\n",
    "        \"secondary_control_title\", \"secondary_description\"\n",
    "    ],\n",
    "    template=MATCHING_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helper Functions\n",
    "# -----------------------------\n",
    "def load_controls(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path).fillna(\"\")\n",
    "\n",
    "def control_to_text(row) -> str:\n",
    "    return f\"{row['domain']} {row['sub_domain']} {row['control_number']} {row['control_title']} {row['description']}\"\n",
    "\n",
    "def build_faiss_index(controls: pd.DataFrame):\n",
    "    vectors = [get_embedding(control_to_text(row)) for _, row in tqdm(controls.iterrows(), total=len(controls), desc=\"üîç Embedding Secondary\")]\n",
    "    dim = len(vectors[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vectors).astype(\"float32\"))\n",
    "    return index, vectors\n",
    "\n",
    "# -----------------------------\n",
    "# Matching Logic\n",
    "# -----------------------------\n",
    "def map_controls(primary_df, secondary_df, top_k=TOP_K):\n",
    "    secondary_index, _ = build_faiss_index(secondary_df)\n",
    "    used_secondary = set()\n",
    "    output_rows = []\n",
    "\n",
    "    for i, primary in tqdm(primary_df.iterrows(), total=len(primary_df), desc=\"üß† Matching\"):\n",
    "        primary_vec = np.array([get_embedding(control_to_text(primary))], dtype=\"float32\")\n",
    "        distances, indices = secondary_index.search(primary_vec, top_k)\n",
    "\n",
    "        matched = False\n",
    "        for idx in indices[0]:\n",
    "            if idx in used_secondary:\n",
    "                continue\n",
    "\n",
    "            candidate = secondary_df.iloc[idx]\n",
    "            formatted_prompt = match_prompt.format(\n",
    "                primary_domain=primary[\"domain\"],\n",
    "                primary_sub_domain=primary[\"sub_domain\"],\n",
    "                primary_control_number=primary[\"control_number\"],\n",
    "                primary_control_title=primary[\"control_title\"],\n",
    "                primary_description=primary[\"description\"],\n",
    "                secondary_domain=candidate[\"domain\"],\n",
    "                secondary_sub_domain=candidate[\"sub_domain\"],\n",
    "                secondary_control_number=candidate[\"control_number\"],\n",
    "                secondary_control_title=candidate[\"control_title\"],\n",
    "                secondary_description=candidate[\"description\"]\n",
    "            )\n",
    "\n",
    "            response = chat_model.invoke(formatted_prompt).content.strip().lower()\n",
    "\n",
    "            if \"yes\" in response:\n",
    "                matched = True\n",
    "                used_secondary.add(idx)\n",
    "                output_rows.append({\n",
    "                    f\"{PRIMARY_PREFIX}_domain\": primary[\"domain\"],\n",
    "                    f\"{PRIMARY_PREFIX}_sub_domain\": primary[\"sub_domain\"],\n",
    "                    f\"{PRIMARY_PREFIX}_control_number\": primary[\"control_number\"],\n",
    "                    f\"{PRIMARY_PREFIX}_control_title\": primary[\"control_title\"],\n",
    "                    f\"{PRIMARY_PREFIX}_description\": primary[\"description\"],\n",
    "                    f\"{SECONDARY_PREFIX}_domain\": candidate[\"domain\"],\n",
    "                    f\"{SECONDARY_PREFIX}_sub_domain\": candidate[\"sub_domain\"],\n",
    "                    f\"{SECONDARY_PREFIX}_control_number\": candidate[\"control_number\"],\n",
    "                    f\"{SECONDARY_PREFIX}_control_title\": candidate[\"control_title\"],\n",
    "                    f\"{SECONDARY_PREFIX}_description\": candidate[\"description\"]\n",
    "                })\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            output_rows.append({\n",
    "                f\"{PRIMARY_PREFIX}_domain\": primary[\"domain\"],\n",
    "                f\"{PRIMARY_PREFIX}_sub_domain\": primary[\"sub_domain\"],\n",
    "                f\"{PRIMARY_PREFIX}_control_number\": primary[\"control_number\"],\n",
    "                f\"{PRIMARY_PREFIX}_control_title\": primary[\"control_title\"],\n",
    "                f\"{PRIMARY_PREFIX}_description\": primary[\"description\"],\n",
    "                f\"{SECONDARY_PREFIX}_domain\": \"\",\n",
    "                f\"{SECONDARY_PREFIX}_sub_domain\": \"\",\n",
    "                f\"{SECONDARY_PREFIX}_control_number\": \"\",\n",
    "                f\"{SECONDARY_PREFIX}_control_title\": \"\",\n",
    "                f\"{SECONDARY_PREFIX}_description\": \"\"\n",
    "            })\n",
    "\n",
    "    for j, row in secondary_df.iterrows():\n",
    "        if j not in used_secondary:\n",
    "            output_rows.append({\n",
    "                f\"{PRIMARY_PREFIX}_domain\": \"\",\n",
    "                f\"{PRIMARY_PREFIX}_sub_domain\": \"\",\n",
    "                f\"{PRIMARY_PREFIX}_control_number\": \"\",\n",
    "                f\"{PRIMARY_PREFIX}_control_title\": \"\",\n",
    "                f\"{PRIMARY_PREFIX}_description\": \"\",\n",
    "                f\"{SECONDARY_PREFIX}_domain\": row[\"domain\"],\n",
    "                f\"{SECONDARY_PREFIX}_sub_domain\": row[\"sub_domain\"],\n",
    "                f\"{SECONDARY_PREFIX}_control_number\": row[\"control_number\"],\n",
    "                f\"{SECONDARY_PREFIX}_control_title\": row[\"control_title\"],\n",
    "                f\"{SECONDARY_PREFIX}_description\": row[\"description\"]\n",
    "            })\n",
    "\n",
    "    return output_rows\n",
    "\n",
    "# -----------------------------\n",
    "# Main Execution\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    primary_df = load_controls(PRIMARY_CSV)\n",
    "    secondary_df = load_controls(SECONDARY_CSV)\n",
    "\n",
    "    mapped_rows = map_controls(primary_df, secondary_df)\n",
    "    output_df = pd.DataFrame(mapped_rows)\n",
    "    output_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\n‚úÖ Mapping completed and saved to: {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
